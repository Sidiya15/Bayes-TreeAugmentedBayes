# Bayes-TreeAugmentedBayes

This project was conducted as a part of the evaluation of the graphical models class. The idea was to put in evidance the efficiency of the Tree Augmented Naive Bayes 
algorithm (TAN) over the Naive Bayes algorithm (NB) in the gaussian case. The idea was that if Naive Bayes, despite its incorrect independence assumption, works well, so taking into account an eventual correlations between features, the performance of the classification can only be improved. This model maintains the structure of the Naive Bayes graph and increases it by adding edges between the variables in order to capture the correlations between some of them. By constraint of complexity, the TAN imposes a restriction on the level of interaction between the variables to one. Therefore, in a TAN graph, each variable can have two parents which are the class node and another variable node, except for the root variable which has only the class node as parent. Another important element in the conception of the TAN is the choice of the tree construction. For such purpose, we chose to use the Kruskal algorithm to construct the maximum spanning tree with regard to the mutual information between each pair of features which, in the case of normality assumption, goes back to computing the quantity <b>I(X, Y)= 0.5*log(1-p<sup>2</sup>)</b> where <b>p</b> is the Pearson correlation coefficient between (X, Y).

You find more of this detail in the notebook above and the decision rule for each algorithm. Once we finished the implementation, we put the two models in action for two datasets that are <b>iris</b> and <b>digits</b> both from the scikit-learn library. The TAN algorithm well outperformed the NB for digits dataset having an accuracy of 94% when NB had 88% and slightly for the iris dataset since the NB performed well on this dataset having 98% of accuracy and the TAN had 99% (More specificaly, the NB misclassified two instances of the test set while the TAN only misclassified one)
